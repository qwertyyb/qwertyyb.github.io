<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java版本更新后,eclipse打不开]]></title>
    <url>%2F2017%2F11%2F02%2Fjava%E7%89%88%E6%9C%AC%E6%9B%B4%E6%96%B0%E5%90%8E-eclipse%E6%89%93%E4%B8%8D%E5%BC%80%2F</url>
    <content type="text"><![CDATA[在安装 jdk 的时候，安装成功后会默认询问是否安装 jre ，很多人是一路默认，稀里糊涂就安装上了。今天就遇到了这个问题，jre 环境更新后，eclipse 打不开了，报打不到 jre 的环境了，经过研究，打到了解决办法，这里记录一下。 解决办法在 C:\Users\{你的用户名}\eclipse\java-oxygen\eclipse 目录下，找到 eclipse.ini 文件，找到 -vm 行的下一行，把路径改为jdk下的 jre/bin 这样，以后 jre 再升级的时候也不会出错了 出错的原因在安装jdk后，java目录会出现两个文件夹，jdk和jre, 具体的目录名字，还会包括版本号，当初始化eclipse的时候，会用jre的目录初始化，而当jre升级后，原来的版本更改，目录名也变了，自然就找不到原来的环境目录了 参考连接：更详细的原理请参考这里： 安装JDK的时候为什么会有两个jre文件]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java, eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks服务端的配置]]></title>
    <url>%2F2017%2F05%2F28%2Fshadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[基于centOS7安装组件123$ yum install m2crypto python-setuptools$ easy_install pip$ pip install shadowsocks 安装完成后配置服务器参数1$ vi /etc/shadowsocks.json 写入如下配置：1234567891011&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 443, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;: 1080, &quot;password&quot;: &quot;mypassword&quot;, &quot;timeout&quot; 300, &quot;method&quot;: &quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false, &quot;workers&quot;: 1&#125; 将上面的mypassword替换成你的密码， server_port也是可以修改的， 例如443是Shadowsocks客户端默认的端口号如果需要修改端口，需要在防火墙里打开相应的端口，用firewalld操作就简单了1$ vi /usr/lib/firewalld/services/ss.xml 把下面的代码粘贴到里面1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;service&gt; &lt;short&gt;SS&lt;/short&gt; &lt;description&gt;Shadowsocks port &lt;/description&gt; &lt;port protocol=&quot;tcp&quot; port=&quot;443&quot;/&gt;&lt;/service&gt; port可自定义，但是需要跟上面的server_port对应起来，保存退出，然后重启firewalld服务12$ firewall-cmd --permanent --add-service=ss$ firewall-cmd --reload 运行命令，启动Shadowsocks服务1$ ssserver -c /etc/shadowsocks.json 至此shadowsocks搭建完成，shadowsocks已经可以使用，如果你没有过高的要求，下面的步骤可以省略，下面是后台运行Shadowsocks的步骤。 安装supervisor实现后台运行运行以下命令下载supervisor1$ easy_install supervisor 然后创建配置文件1$ echo_supervisord_conf &gt; /etc/supervisord.conf 然后修改配置文件1$ vi /etc/supervisord.conf 在文件末尾添加12345[program:ssserver]command = ssserver -c /etc/shadowsocks.jsonautostart = trueautorestart = truestartsec = 3 设置 supervisord 开机启动1$ vi /etc/rc.local 在末尾另起一行添加1$ supervisord 保存退出（和上文类似）。另 CentOS7 还需要为 rc.local 添加执行权限1$ chmod +x /etc/rc.local 至此运用 supervisord 控制 Shadowsocks 开机自启和后台运行设置完成。重启服务器即可P.S. 如果当你在运行 supervisord 命令时出现以下的错误提示123&apos;Supervisord is running as root and it is searching &apos;Error: Another program is already listening on a port that one of our HTTP Servers is configured to use. Shut this program down before starting supervisord.For help, use /usr/bin/supervisord -h 那是因为 supervisord 已经启动了，重复启动就会出现上面的错误提示]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux, shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MP3文件分析之ID3v2.3版本]]></title>
    <url>%2F2017%2F03%2F18%2FMP3%E6%96%87%E4%BB%B6%E5%88%86%E6%9E%90%E4%B9%8BID3v2-3%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[MP3文件分析之ID3v2.3版本关于读取MP3文件的ID3标签，网上众说纷芸，但很多都是错的，在这里总结一下。所有的分析都基于ID3官方网站www.id3.org 1. 标签头 标签头有十个字节，在文件最开始的10个字节，它的数据结构如下： 123456789char Header[3];//这个字符串一定为&quot;ID3&quot;char version;//版本号，而针对在下要讲的版本，理应为3.即为ID3v2.3char revision;//副版本号，好像一直都是0，没看到过它有变过char flags;//一些特殊的消息标记，只会使用此字节的高3位，其它的五位并没有什么卵用char size[4];//代表整个标签帧的大小，但是不包括这开始的10个字节，所以这里得到的size需要加上10才代表整个标签帧的大小 ID3v2 flags中的%abc00000，其中高三位表示如下： 12345678a - Unsynchronisation表示是否同步(自己乱翻译的)，这个搞不清是什么鬼，个人英语不是很行，大概是为了数据帧同步帧数据,校正数据用的b - Extended header表示是否有扩展头部，这个扩展头部是用来补充标签信息的，原文如下：The extended header contains information that is not vital to the correct parsing of the tag information, hence the extended header is optional. c - Experimental indicator表示是否为试验测试，这个东西是什么鬼也不知道，没见过MP3音乐文件这个位进行了设置 ID3v2 size中的4?%0xxxxxxx表示的是4个字节，后面的%0xxxxxxx就是一个字节8位了。 然后计算标签帧的大小，ID3规定这四个字节中每个字节的最高位恒为0不使用，格式如下： 1var size = size4 &amp; 0x7f | ((size3 &amp; 0x7f) &lt;&lt; 7) | ((size2 &amp; 0x7f) &lt;&lt; 14) | ((size1 &amp; 0x7f) &lt;&lt; 21); 注意：这里的帧大小，并不包含帧头的10个字节，只表示帧内容的大小 这里再说一个特殊消息标记的Extended header处理，当Extended header这个标记位设置为1时，在这最开始的10个字节后面会增加有Extended header的内容，这部分内容非常有意思，因为它所占用的大小不算在之前10个字节的size中，就相当这里会凭空多出一些字节。 然后这个Extended header信息内容格式如下： 1234Extended header size $xx xx xx xxExtended Flags $xx xxSize of padding $xx xx xx xx... Extended header size有四个字节，表示接下来的数据占用多少个字节。 Extended Flags 这两个字节不知道干什么 接下来就都是扩展头部的数据了（我猜Extended Flags这两个字节好像没有，扩展头部本来就没有多大用，一般直接就滤掉了） 这里是JS代码实现标签头的识别： 1234567891011121314151617181920212223242526272829303132333435getByteAt(iOffset);//得到iOffset位置的一个字节数据isBitSetAt(iOffset, iBit);//判断iOffset位置的字节的iBit位是1还是0readSynchsafeInteger32At(data, iOffset);//这是处理标签头的sizegetLongAt(iOffset, bBigEndian);//得到iOffset位置的Long数据，bBigEndian表示是低端还是高端/****************/var offset = 0, major = data.getByteAt(offset + 3), revision = data.getByteAt(offset + 4), unsynch = data.isBitSetAt(offset + 5, 7), xheader = data.isBitSetAt(offset + 5, 6), xtest = data.isBitSetAt(offset + 5, 5), size = this.readSynchsafeInteger32At(data, offset + 6);offset += 10;if (xheader) &#123; var xheadersize = data.getLongAt(offset, true); offset += xheadersize + 4;&#125;var id3 = &#123; &quot;version&quot;: &apos;2.&apos; + major + &apos;.&apos; + revision, &quot;major&quot;: major, &quot;flags&quot;: &#123; &quot;unsynchronisation&quot;: unsynch, &quot;extended_header&quot;: xheader, &quot;experimental_indicator&quot;: xtest &#125;, &quot;size&quot;: size&#125;; 2. 标签帧内容帧头的定义12345char ID[4];//用四个字符标识一个帧，表明这个帧的内容是什么char size[4];//帧内容的大小，不包括帧头char flags[2];//特殊的消息标记 帧标识：The frame ID made out of the characters capital A-Z and0-9.FrameID会是一串由A-Z和0-9的字符串组成，占用4个字节 帧大小：The frame ID is followed by a size descriptor, making a total header size of ten bytes in every frame. The size is calculated as frame size excluding frame header 最后这个flags跟前面说的都一样为特殊标记 常见有用的帧标识 TIT2：歌曲标题名字 TPE1：作者名字 TALB：作品专辑 TYER：作品产生年代 COMM：备注信息 APIC：专辑图片 帧标记说明只定义了6位，另外的10位为0，一般这些标记也不用，通常为0，格式如下： 12345678910111213flags %abc00000 ijk00000 a -- 标签保护标志，设置时认为此帧作废b -- 文件保护标志，设置时认为此帧作废c -- 只读标志，设置时认为此帧不能修改(但我没有找到一个软件理会这个标志) i -- 压缩标志，设置时一个字节存放两个BCD 码表示数字 j -- 加密标志(没有见过哪个MP3 文件的标签用了加密) k -- 组标志，设置时说明此帧和其他的某帧是一组 帧标识帧标识这一块有太多，各种各样的，到官方网站去看，这里会主要区分三种主要的标识信息（其他的都拜拜吧，通过看官方网站的信息你就知道为什么拜拜了）。 T*，即以T开头的帧标识，为文本标识。 文本标识就会涉及到文字的编码，此标签内容分为三部分。 第一部分为1个字节，这个字节一定是[0x00,0x01,0x02,0x03]中的一种，0x00代表这个标签帧后续的数据为iso-8859-1编码，0x01则是utf-16编码，0x02则是utf-16be编码，0x03则是utf-8编码 第二部分根据编码确定是否存在如果为0x00编码的话就不会存在，字节就是直接读取 如果为0x01和0x02那么这里会占用2个字节，会出现两种可能的数据，一种为FF FE表示小端，即数据存储是高数据在高位，一种为FE FF表示大端与小端相反 如果为0x03编码则是会占用三个字节 EF BB BF 第三部分就是数据 APIC，专辑图片，好像整个MP3的数据就只有这个标识有图片 这里直接以官方说明来讲解比较好： 123456&lt;Header for &apos;Attached picture&apos;, ID: &quot;APIC&quot;&gt;Text encoding $xxMIME type &lt;text string&gt; $00Picture type $xxDescription &lt;text string according to encoding&gt; $00 (00)Picture data &lt;binary data&gt; 第一个为数据编码，和以T开头的一样，分为四种0x00,0x01,0x02,0x03 第二个为MIME type数据了，表示的是什么类型图片，有image/jpeg,image/png…等，这里的字节数不确定，是用0x00作为字符串的结束标志，来停止读取的，也就是说MIME type数据需要一直读取，知道读取到了0x00也就是我们常见的字符串结束标志\0. 第三个为Picture type，表示的是图片代表什么，是作者还是一些什么内容。 第四个为Description，就是简单的图片描述了，这里和MIME type数据一样，是以\0为结束的，这里多说一句的是，这个属性好像也不经常用，它的值经常为”“ 第五部分就是图片数据了，记住这不是base64编码的数据。 Picture type的值扩充说明(就是这些值表示这张图片的大概内容) 123456789101112131415161718192021$00 Other$01 32x32 pixels &apos;file icon&apos; (PNG only)$02 Other file icon$03 Cover (front)$04 Cover (back)$05 Leaflet page$06 Media (e.g. lable side of CD)$07 Lead artist/lead performer/soloist$08 Artist/performer$09 Conductor$0A Band/Orchestra$0B Composer$0C Lyricist/text writer$0D Recording Location$0E During recording$0F During performance$10 Movie/video screen capture$11 A bright coloured fish$12 Illustration$13 Band/artist logotype$14 Publisher/Studio logotype ​ COMM，备注消息，这个玩意一直在飞，全程都是懵逼的，这个属性感觉并没有什么卵用 这里还是官方说明来讲解比较好 12345&lt;Header for &apos;Comment&apos;, ID: &quot;COMM&quot;&gt;Text encoding $xxLanguage $xx xx xxShort content descrip. &lt;text string according to encoding&gt; $00 (00)The actual text &lt;full text string according to encoding&gt; 第一个不想说了，跟前面的一模一样; 第二个表示接下来是什么语言，就是说是中文还是英文还是其它语言，一般是英文就是eng 然后就是短描叙了，这里的字节数也是不确定的，也就是说这里是以\0为结尾的数据，需要不断读取直到\0结束 接下来就是最终的数据了 3. 编码数据扩充 utf-16，即为UCS-2，这种编码会出现两种形式，一个为2字节也就是一个字，一个为四字节也就是两个字，当第一个字节小于0xD8或者大于0xDF，则是第一种情况，否则就是第二种，其中0xDB-0xDF为代理区当然在这个音乐文件中有小端和大端区分，所以我们经常会看到如下编码选项 UCS-2 Big Endian(大端),UCS-2 Little Endian(小端) utf-8,这个编码可以说是最操蛋的，网上的解释也参差不齐，俺也懒得去看官网了，这里讲解的只是最常用的，大众的。这个编码分为三种，一个为1字节(这里很明显是用一个字节来表示英文字母)，然后就是2字节，接着就是3字节区分： 第一字节小于0x80则为1个字节 第一字节大于等于0xC2小于0xE0则是2字节 第一字节大于等于0xE0小于0xF0则是3字节 其他的编码就一股脑的读取一个字节就可以了 关于更多文字编码的知识可以看这里：彻底搞懂字符编码;]]></content>
      <tags>
        <tag>ID3, Audio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底搞懂字符编码]]></title>
    <url>%2F2017%2F03%2F12%2F%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[​ 编码的相关知识还真是杂乱不堪，不少人对一些知识理解似乎也有些偏差，网上百度,google的内容，也有不少以讹传讹，根本就是错误的（例如说 unicode编码是两个字节），各种软件让你选择编码的时候，常常是很长的一个选单，让用户不知道该如何选。基于这样的问题，我就写下我的理解吧，一方面帮助一些需要帮助的人纠正认识，一方面作为自己以后备查的资料。 字符编码的历史1. ASCII(American Standard Code for Information Interchange)​ 美国信息交换标准代码，这是计算机上最早使用的通用的编码方案。那个时候计算机还只是拉丁文字的专利，根本没有想到现在计算机的发展势头，如果想到了，可能一开始就会使用unicode了。当时绝大部分专家都认为，要用计算机，必须熟练掌握英文。这种编码占用7个Bit，在计算机中占用一个字节，8位，最高位没用，通讯的时候有时用作奇偶校验位。因此ASCII编码的取值范围实际上是：0x00-0x7f,只能表示128个字符。后来发现128个不太够用，做了扩展，叫做ASCII扩展编码，用足八位，取值范围变成：0x00-0xff,能表示256个字符。其实这种扩展意义不大，因为256个字符表示一些非拉丁文字远远不够，但是表示拉丁文字，又用不完。所以扩展的意义还是为了下面的ANSI编码服务。 2. ANSI（American National Standard Institite ）​ 美国国家标准协会，也就是说，每个国家（非拉丁语系国家）自己制定自己的文字的编码规则，并得到了ANSI认可，符合ANSI的标准，全世界在表示对应国家文字的时候都通用这种编码就叫ANSI编码。换句话说，中国的ANSI编码和在日本的ANSI的意思是不一样的，因为都代表自己国家的文字编码标准。比如中国的ANSI对应就是GB2312标准，日本就是JIT标准，香港，台湾对应的是BIG5标准等等。当然这个问题也比较复杂，微软从95开始，用就是自己搞的一个标准GBK。GB2312里面只有6763个汉字，682个符号，所以确实有时候不是很够用。GBK一直能和GB2312相互混淆并且相安无事的一个重要原因是GBK全面兼容GB2312，所以没有出现任何冲突，你用GB2312编码的文件通过GBK去解释一定能获得相同的显示效果，换句话说：GBK对GB2312就是，你有的，我也有，你没得的，我还有！ ​ 好了，ANSI的标准是什么呢，首先是ASCII的代码你不能用！也就是说ASCII码在任何ANSI中应该都是相同的。其他的，你们自己扩展。所以呢，中国人就把ASCII码变成8位，0x7f之前我不动你的，我从0xa0开始编，0xa0到0xff才95个码位，对于中国字那简直是杯水车薪，因此，就用两个字节吧，此编码范围就从0xA1A1 - 0xFEFE，这个范围可以表示23901个汉字。基本够用了吧，GB2312才7000多个呢！GBK更猛，编码范围是从0x8140 - 0xFEFE,可以表示3万多个汉字。可以看出，这两种方案，都能保证汉字头一个字节在0x7f以上，从而和ASCII不会发生冲突。能够实现英文和汉字同时显示。 ​ BIG5，香港和台湾用的比较多，繁体，范围： 0xA140 - 0xF9FE, 0xA1A1 - 0xF9FE，每个字由两个字节组成，其第一字节编码范围为0xA1~0xF9，第二字节编码范围为0x40-0x7E与0xA1-0xFE，总计收入13868个字 (包括5401个常用字、7652 个次常用字、7个扩充字、以及808个各式符号)。 ​ 那么到底ANSI是多少位呢？这个不一定！比如在GB2312和GBK，BIG5中，是两位！但是其他标准或者其他语言如果不够用，就完全可能不止两位！ ​ 例如：GB18030:​ GB18030-2000(GBK2K)在GBK的基础上进一步扩展了汉字，增加了藏、蒙等少数民族的字形。GBK2K从根本上解决了字位不够，字形不足的问题。它有几个特点：它并没有确定所有的字形，只是规定了编码范围，留待以后扩充。编码是变长的，其二字节部分与GBK兼容；四字节部分是扩充的字形、字位，其编码范围是首字节0x81-0xfe、二字节0x30-0x39、三字节0x81-0xfe、四字节0x30-0x39。它的推广是分阶段的，首先要求实现的是能够完全映射到Unicode3.0标准的所有字形。它是国家标准，是强制性的。 ​ 搞懂了ANSI的含义，我们发现ANSI有个致命的缺陷，就是每个标准是各自为阵的，不保证能兼容。换句话说，要同时显示中文和日本文或者阿拉伯文，就完全可能会出现一个编码两个字符集里面都有对应，不知道该显示哪一个的问题，也就是编码重叠的问题。显然这样的方案不好，所以Unicode才会出现！ 3. MBCS（Multi-Byte Chactacter System（Set)）​ 多字节字符系统或者字符集，基于ANSI编码的原理上，对一个字符的表示实际上无法确定他需要占用几个字节的，只能从编码本身来区分和解释。因此计算机在存储的时候，就是采用多字节存储的形式。也就是你需要几个字节我给你放几个字节，比如A我给你放一个字节，比如”中“，我就给你放两个字节，这样的字符表示形式就是MBCS。 ​ 在基于GBK的windows中，不会超过2个字节，所以windows这种表示形式有叫做DBCS（Double-Byte Chactacter System），其实算是MBCS的一个特例。C语言默认存放字符串就是用的MBCS格式。从原理上来说，这样是非常经济的一种方式。 4. CodePage​ 代码页，最早来自IBM，后来被微软，Oracle ,SAP等广泛采用。因为ANSI编码每个国家都不统一，不兼容，可能导致冲突，所以一个系统在处理文字的时候，必须要告诉计算机你的ANSI是哪个国家和地区的标准，这种国家和标准的代号（其实就是字符编码格式的代号），微软称为Codepage代码页，其实这个代码页和字符集编码的意思是一样的。告诉你代码页，本质就是告诉了你编码格式。 ​ 但是不同厂家的代码页可能是完全不同，哪怕是同样的编码，比如， UTF-8字符编码 在IBM对应的代码页是1208，在微软对应的是65001,在德国的SAP公司对应的是 4110 。所以啊，其实本来就是一个东西，大家各自为政，搞那么多新名词，实在没必要！所以标准还是很重要的！！！ ​ 比如GBK的在微软的代码页是936，告诉你代码页是936其实和告诉你我编码格式是GBK效果完全相同。那么处理文本的时候就不会有问题，不会去考虑某个代码是显示的韩文还是中文，同样，日文和韩文的代码页就和中文不同，这样就可以避免编码冲突导致计算机不知如何处理的问题。当然用这个也可以很容易的切换语言版本。但是这都是治标不治本的方法，还是无法解决同时显示多种语言的问题，所以最后还是都用unicode吧，永远不会有冲突了。 5. Unicode(Universal Code)​ 这是一个编码方案，说白了就是一张包含全世界所有文字的一个编码表，不管你用的上，用不上，不管是现在用的，还是以前用过的，只要这个世界上存在的文字符号，统统给你一个唯一的编码，这样就不可能有任何冲突了。不管你要同时显示任何文字，都没有问题。因此在这样的方案下，Unicode出现了。Unicode编码范围是：0-0x10FFFF，可以容纳1114112个字符，100多万啊。全世界的字符根本用不完了，Unicode 5.0版本中，才用了238605个码位。所以足够了。 ​ 因此从码位范围看，严格的unicode需要3个字节来存储。但是考虑到理解性和计算机处理的方便性，理论上还是用4个字节来描述。Unicode采用的汉字相关编码用的是《CJK统一汉字编码字符集》— 国家标准 GB13000.1 是完全等同于国际标准《通用多八位编码字符集 (UCS)》 ISO 10646.1。《GB13000.1》中最重要的也经常被采用的是其双字节形式的基本多文种平面。在这65536个码位的空间中，定义了几乎所有国家或地区的语言文字和符号。其中从0x4E00到 0x9FA5 的连续区域包含了 20902 个来自中国（包括台湾）、日本、韩国的汉字，称为 CJK (Chinese Japanese Korean) 汉字。CJK是《GB2312-80》、《BIG5》等字符集的超集。 CJK包含了中国，日本，韩国，越南，香港，也就是CJKVH。这个在UNICODE的Charset chart中可以明显看到。 unicode的相关标准可以从unicode.org上面获得，目前已经进行到了6.0版本。 下面这段描述来自百度百科： Unicode字符集可以简写为UCS（Unicode Character Set）。早期的 unicodeUnicode标准有UCS-2、UCS-4的说法。UCS-2用两个字节编码，UCS-4用4个字节编码。UCS-4根据最高位为0的最高字节分成2^7=128个group。每个group再根据次高字节分为256个平面（plane）。每个平面根据第3个字节分为256行 （row），每行有256个码位（cell）。group 0的平面0被称作BMP（Basic Multilingual Plane）。将UCS-4的BMP去掉前面的两个零字节就得到了UCS-2。每个平面有2^16=65536个码位。Unicode计划使用了17个平面，一共有1765536=1114112个码位。在Unicode 5.0.0版本中，已定义的码位只有238605个，分布在平面0、平面1、平面2、平面14、平面15、平面16。其中平面15和平面16上只是定义了两个各占65534个码位的专用区（Private Use Area），分别是0xF0000-0xFFFFD和0x100000-0x10FFFD。所谓专用区，就是保留给大家放自定义字符的区域，可以简写为PUA。 平面0也有一个专用区：0xE000-0xF8FF，有6400个码位。平面0的0xD800-0xDFFF，共2048个码位，是一个被称作代理区Surrogate）的特殊区域。代理区的目的用两个UTF-16字符表示BMP以外的字符。在介绍UTF-16编码时会介绍。如前所述在Unicode 5.0.0版本中，238605-655342-6400-2408=99089。余下的99089个已定义码位分布在平面0、平面1、平面2和平面14上，它们对应着Unicode目前定义的99089个字符，其中包括71226个汉字。平面0、平面1、平面2和平面14上分别定义了52080、3419、43253和337个字符。平面2的43253个字符都是汉字。平面0上定义了27973个汉字。 Unicode编码1. Unicode的实现方案Unicode其实只是一张巨大的编码表。要在计算机里面实现，也出现了几种不同的方案。也就是说如何表示unicode编码的问题，下面是几种常见的方案： (1) UTF-8（UCS Transformation Format 8bit)​ 这个方案的意思以8位为单位来标识文字，注意并不是说一个文字用8位标识。他其实是一种MBCS方案，可变字节的。到底需要几个字节表示一个符号，这个要根据这个符号的unicode编码来决定，最多4个字节。 编码规则如下： Unicode编码（16进制） UTF-8字节流（二进制） 000000 - 00007F 0xxxxxxx 000080 - 0007FF 110xxxxx 10xxxxxx 000800 - 00FFFF 1110xxxx 10xxxxxx 10xxxxxx 010000 - 10FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx ​ UTF-8的特点是对不同范围的字符使用不同长度的编码。对于0x00-0x7F之间的字符，UTF-8编码与ASCII编码完全相同。 ​ UTF-8编码的最大长度是4个字节。从上表可以看出，4字节模板有21个x，即可以容纳21位二进制数字。Unicode的最大码位0x10FFFF也只有21位。 ​ 例1：“汉”字的Unicode编码是0x6C49。0x6C49在0x0800-0xFFFF之间，使用用3字节模板了：1110xxxx 10xxxxxx 10xxxxxx。将0x6C49写成二进制是：0110 1100 0100 1001， 用这个比特流依次代替模板中的x，得到：11100110 10110001 10001001，即E6 B1 89。 ​ 例2：Unicode编码0x20C30在0x010000-0x10FFFF之间，使用用4字节模板了：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx。 ​ 将0x20C30写成21位二进制数字（不足21位就在前面补0）：0 0010 0000 1100 0011 0000，用这个比特流依次代替模板中的x，得到：11110000 10100000 10110000 10110000，即F0 A0 B0 B0。 (2) UTF-16 UTF-16编码以16位无符号整数为单位。注意是16位为一个单位，不表示一个字符就只有16位。现在机器上的unicode编码一般指的就是UTF-16。绝大部分2个字节就够了，但是不能绝对的说所有字符都是2个字节。这个要看字符的unicode编码处于什么范围而定，有可能是2个字节，也可能是4个字节。这点请注意！ 下面算法解释来自百度百科。 我们把Unicode unicode编码记作U。编码规则如下： ​ 如果U&lt;0x10000，U的UTF-16编码就是U对应的16位无符号整数（为书写简便，下文将16位无符号整数记作WORD）。如果U≥0x10000，我们先计算U’=U-0x10000，然后将U’写成二进制形式：yyyy yyyy yyxx xxxx xxxx，U的UTF-16编码（二进制）就是：110110yyyyyyyyyy 110111xxxxxxxxxx。为什么U’可以被写成20个二进制位？Unicode的最大码位是0x10ffff，减去0x10000后，U’的最大值是0xfffff，所以肯定可以用20个二进制位表示。 ​ 例如：Unicode编码0x20C30，减去0x10000后，得到0x10C30，写成二进制是：0001 0000 1100 0011 0000。用前10位依次替代模板中的y，用后10位依次替代模板中的x，就得到：1101100001000011 1101110000110000，即0xD8430xDC30。 ​ 按照上述规则，Unicode编码0x10000-0x10FFFF的UTF-16编码有两个WORD，第一个WORD的高6位是110110，第二个WORD的高6位是110111。可见，第一个WORD的取值范围（二进制）是11011000 00000000到11011011 11111111，即0xD800-0xDBFF。第二个WORD的取值范围（二进制）是11011100 00000000到11011111 11111111，即0xDC00-0xDFFF。为了将一个WORD的UTF-16编码与两个WORD的UTF-16编码区分开来，Unicode编码的设计者将0xD800-0xDFFF保留下来，并称为代理区（Surrogate）： 高位替代(High Surrogates) 高位专用替代(High Private Use Surrogates) 低位替代(Low Surrogates) D800－DB7F DB80－DBFF DC00－DFFF ​ 高位替代就是指这个范围的码位是两个WORD的UTF-16编码的第一个WORD。低位替代就是指这个范围的码位是两个WORD的UTF-16编码的第二个WORD。那么，高位专用替代是什么意思？我们来解答这个问题，顺便看看怎么由UTF-16编码推导Unicode编码。 如果一个字符的UTF-16编码的第一个WORD在0xDB80到0xDBFF之间，那么它的Unicode编码在什么范围内？我们知道第二个WORD的取值范围是0xDC00-0xDFFF，所以这个字符的UTF-16编码范围应该是0xDB80 0xDC00到0xDBFF 0xDFFF。我们将这个范围写成二进制： 1101101110000000 11011100 00000000 - 1101101111111111 1101111111111111 按照编码的相反步骤，取出高低WORD的后10位，并拼在一起，得到 1110 0000 0000 0000 0000 - 1111 1111 11111111 1111 即0xe0000-0xfffff，按照编码的相反步骤再加上0x10000，得到0xf0000-0x10ffff。这就是UTF-16编码的第一个WORD在0xdb80到0xdbff之间的Unicode编码范围，即平面15和平面16。因为Unicode标准将平面15和平面16都作为专用区，所以0xDB80到0xDBFF之间的保留码位被称作高位专用替代。 (2) UTF-32这个就简单了，和Unicode码表基本一一对应，固定四个字节。为什么不采用UTF-32呢，因为unicode定义的范围太大了，其实99%的人使用的字符编码不会超过2个字节，所以如同统一用4个字节，简单倒是简单了，但是数据冗余确实太大了，不好，所以16位是最好的。就算遇到超过16位能表示的字符，我们也可以通过上面讲到的代理技术，采用32位标识，这样的方案是最好的。所以现在绝大部分机器实现unicode还是采用的utf-16的方案。当然也有UTF-8的方案。比如windows用的就是UTF16方案，不少Linux用的就是utf8方案。 2. 编码存储差异这里就要引出两个名词：LE（little endian):小字节字节序，意思就是一个单元在计算机中的存放时按照低位在前（低地址），高位在后（高地址）的模式存放。 BE（big endian):大字节字节序，和LE相反，是高位在前，低位在后。 ​ 比如一个unicode编码为：0x006C49，如果是LE，那么在文件中的存放顺序应该是：49 6c 00如果是BE ,那么顺序应该是：00 6c 49 ​ 为了识别一个编码过的字符的存储顺序，必须用特殊字符来指示。Unicode字符中U+FEFF被用来指示这种存储顺序，被称作Byte Order Mark（BOM）。 ​ BOM在Big-Endian系统上存储为FE FF；而在Big-Endian系统上存储则为FF FE。所以在以Big-Endian存储的UTF-16（UTF-16BE）的文件的开头，用FEFF指示；以Little-Endian存储的UTF-16（UTF-16LE）的文件的开头，用FFFE指示。 ​ BOM的UTF-8编码为11101111 1011101110111111 (EF BB BF)，所以一般EF BB BF被放在文本的开头，用来指示其编码为UTF-8。 3. Unicode编码实践​ 在Windows的文本编辑工具记事本上，选择“另存为”的时候，用户可以选择不同的编码选项，对应编码选项有“ANSI”，“Unicode”，“Unicode big endian”，以及“UTF-8”。因为Windows的存储方式是Little-Endian，所以“Unicode”，“Unicode big endian”对应的分别是UTF-16LE和UTF-16BE。 ​ 读者可以试着编写一串字符，然后分别用不同的编码保存，再用可以16进制编写的纯文本编辑工具（如，Ultra-edit）来检验一下具体的编码实现和存储顺序。下面是笔者将“田海立（U+7530, U+6D77, U+7ACB）”以不同编码方式保存，得到的结果： 1234567891011田海立_UTF-16BE.txt FEFF75306D777ACB田海立_UTF-16LE.txt FFFE3075776DCB7A田海立_UTF-8.txt EFBBBFE794B0E6B5B7E7AB8B ​ 为了明确起见，BOM的编码用粗体标注；田的编码用红色标注；海的编码用绿色标注；立的编码用蓝色标注。可以看到，记事本（Notepad）存储的Unicode编码的文件的开头位置，用BOM的相应编码指示了编码格式。]]></content>
      <categories>
        <category>编程, 编码</category>
      </categories>
      <tags>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在github上把master分支的内容作为Github Pages站点]]></title>
    <url>%2F2017%2F03%2F11%2FSimpler-Github-Pages-publishing%2F</url>
    <content type="text"><![CDATA[在github上把master分支的内容作为Github Pages站点，有两种方法 方法一把整个master分支或master分支的/docs目录作为GitHub Pages 第一步把代码传到github远程仓库 第二步如果把整个master分支作为Github Pages站点，在settings下Github Pages的source中选择master branch 如果把master分支的/docs目录作为Github Pages站点，则在setting下Github Pages的source中选择master branch /docs folder 参考链接：Simpler Github Pages publishing 方法二如果你的确想用master分支的其他目录作为Github Pages站点（比如说/dist） 第一步/dist 目录需要被 git 记录，于是后面我们才可以用它作为子树（subtree），因此 /dist 不能被 .gitignore规则排除,并且上传到远端仓库。 第二步 git subtree push --prefix dist origin gh-pages 搞定。其中： dist 代表子树所在的目录名origin 是 remote namegh-pages 是目标分支名称]]></content>
      <categories>
        <category>项目管理</category>
      </categories>
      <tags>
        <tag>github, Github Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[辨析 Sass 中的 Map 和 List]]></title>
    <url>%2F2015%2F10%2F21%2Fdemo%2F</url>
    <content type="text"><![CDATA[如果你使用过 Sass 3.3 之前的版本，那么你一定对那段时光颇有感触，那时候没有现如今这么好的条件，那时候的 Map 还只能用多重列表（lists of list）来模拟。多重列表可以实现复杂数据的嵌套定义，但却不是以键值对的形式实现的，所有当我们需要获取其中特定的某一项时就会比较麻烦。Map 这种数据类型天生就是基于键值对的形式，非常便于组织数据。 自从可以使用 Map 之后，开发者们开始毫无顾忌地定义 Map 存储数据，比如断点宽度、颜色值、栅格布局等等响应式排版的细节，都被一股脑的塞进了 Map 中。 那么，有了 Map 之后，我们还有必要使用 List 吗？可能某些人会觉得为了保持向后兼容应该继续使用多重列表模拟 Map，因为可能有些开发者仍然在使用老版本的 Sass 编译器，但实际上，这是多此一举了，Sass 的版本通常由 package.json 或者其他同类型的项目配置文件所控制，往往只需一条命令（gem update sass）即可更新 Sass 的版本，因此基本上无需考虑对老版本的兼容问题。 使用多重列表替代 Map 的优势之一就是减少代码量。下面让我们来比较一下多种列表和 Map 的语法结构以及遍历方式。 测试表格 Variable Description site Sitewide information. page Page specific information and custom variables set in front-matter. config Site configuration theme Theme configuration. Inherits from site configuration. _ (single underscore) Lodash library path Path of current page url Full URL of current page env Environment variables 语法比较 测试标题 在下面的示例中，我创建了一个用于控制响应式布局的数据，该数据一共有四个断点，每一个断点都包含了 min-width、max-width、font-size 和 line-height 四个样式。 Map 语法下面就是使用 Map 存储的数据，具体来说，该 Map 中首先存储了四个用于标识断点的 Key，相对应的是保存具体属性值得 Value。虽然这种形式可读性更高，但是总体代码量却高达 26 行 450 个字符。 1234567891011121314151617181920212223242526$breakpoint-map: ( small: ( min-width: null, max-width: 479px, base-font: 16px, vertical-rhythm: 1.3 ), medium: ( min-width: 480px, max-width: 959px, base-font: 18px, vertical-rhythm: 1.414 ), large: ( min-width: 960px, max-width: 1099px, base-font: 18px, vertical-rhythm: 1.5 ), xlarge: ( min-width: 1100px, max-width: null, base-font: 21px, vertical-rhythm: 1.618 )); 多重列表语法下面的多重列表存储了和上面 Map 同样的数据，在多重列表中没有 Key-Value 的对应关系，这意味着要想找到特定的值，必须使用遍历或 nth() 的方式来实现了。从另一个角度来看，多种列表又比 Map 的代码量小得多，总共只有六行 180 个字符。 123456$breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, null, 21px, 1.618)); 遍历比较 测试标题 从上面简单地比较中可以粗略的看出，多种列表的代码量明显少于 Map。但是，如果我们需要遍历这些值得话，复杂度又是怎样的呢？ 遍历 Map我们可以使用如下的代码遍历 Map： 1@each $label, $map in $breakpoint-map &#123;&#125; 这里的变量 $label 和 $map 会随着对 $breakpoint-map 的遍历被动态地赋值，$label 将会被赋值为 $breakpoint-map 的 Key，而 $map 会被赋值为 $breakpoint-map 的 Value。为了在遍历过程中获取特定值，我们就需要使用 Sass 原生的 map-get() 函数，使用该函数需要传入两个参数：Map 的名字和求取的 Key，最后返回该 Map 中匹配该 Key 的 Value。 具体的做法就是使用 @each 遍历 Map，然后使用 map-get() 获取特定值，最终只需要六行代码 220 个字符即可完成整个遍历： 123456@each $label, $map in $breakpoint-map &#123; $min-width: map-get($map, min-width); $max-width: map-get($map, max-width); $base-font: map-get($map, base-font); $vertical-rhythm: map-get($map, vertical-rhythm);&#125; 遍历多重列表遍历多重列表不必像遍历 Map 一样动态获取到 Map 后再使用 map-get() 函数取特定值，直接遍历一遍即可获得特定值。 因为多种列表内层的每一个列表结构相同，都有按照相同顺序排列的五个值，所以我们可以持续遍历每个值并赋值给特定的变量。无需调用 map-get()，直接引用这些变量即可进行赋值等裸机操作。最终遍历多重列表只使用了两行代码 100 个字符： 12@each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123;&#125; 慎用多重列表 测试标题 经过上述的比对，看起来多重列表各方面都在碾压 Map，实则不然，Sass 中添加 Map 有一条非常重要的原因就是：Key-Value 的映射关系。 遗漏键值如果要使用多重列表，那么就必须保证自己非常熟悉多重列表内部的每一项所代表的意义。下面我们举个例子，来看看遗漏了某些值的情况： 1234567891011121314151617181920$breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, 21px, 1.618));p &#123; @each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123; @if $min-width &#123; @include breakpoint( $min-width ) &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125; @else &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125;&#125; 当我们尝试运行这段代码时，结果肯定是错误地，因为在 $breakpoint-list 的最后一行，xlarge 被赋值给了 $label，1100px 被赋值给了 $min-width，21px 被赋值给了 $max-width, 1.618 被赋值给了 $base-font，最终导致 $vertical-rhythm 没有被赋值，结果就是 font-size 的属性值是错的，line-height 的属性值是空的。此外，Sass 还不会对此抛出错误，导致我们无从知晓错误所在。 如果我们使用 Map 来代替这里的多重列表，那么使用 map-get() 函数即使遇见空值也能正确获得想要的结果。这就是值得我们慎重思考的地方：多种列表虽然简单快速，但是丧失了 Map 中的容错能力和快速取值能力。 查找特定列表在多重列表中查找特定列表简直就是一种折磨。如果使用 Map，那么配合 map-get() 函数可以快速定位到特定子 Map： 1$medium-map: map-get($maps, medium); 但如果要获取多种列表 medium 列表，麻烦可就大了： 123456789@function get-list($label) &#123; @each $list in $breakpoint-list &#123; @if nth($list, 1) == $label &#123; @return $list; &#125; &#125; @return null;&#125;$medium-list: get-list(medium); 这段代码的逻辑就是遍历整个多重列表，知道找到第一个匹配项，然后返回，如果一直没有找到匹配项，就一直遍历到末尾，然后返回 null。这实际上就是手工实现了 map-get() 的逻辑。 缺少原生的 Map 函数Sass 提供了诸多的原生函数用于处理 Map 数据类型，但是多重列表是没法调用这些函数的，比如，使用 map-merge() 可以合并两个 Map，如果两个 Map 有相同的值，则取第二个 Map 的值为最终值。当然你也可以在多重列表中使用 join() 或 append() 来增加新列表，从而模拟出 map-merge() 的效果。 另一个实用的 Map 函数就是 map-has-key()，对于依赖 map-get() 的自定义函数来说，map-has-key() 可以用来验证特定的 Key 是否存在。但在列表中是完全没有相似的方法。 总结 Test Title 相比起列表来说，Key-Value 模型的 Map 显然更有力量，原生的 Sass Map 函数更是提供了强力的数据查找和验证工具。 虽然多重列表代码量少，但并不能像 Map 一样进行错误检查或验证参数。在大多数时候，相比较多重列表而言，我相信 Map 是更好的选择。如果是为了更少的代码量和其他简单地调用，那么我偶尔会用用多重列表，但是从项目的宏观控制和数据存储方面显然更优秀。]]></content>
      <tags>
        <tag>css</tag>
      </tags>
  </entry>
</search>
